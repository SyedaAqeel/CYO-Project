---
title: "Maternal_Health_Risk_Prediction"
author: "Syeda Aqeel"
date: "3/7/2022"
output:
  pdf_document: default
---


#Introduction


Maternal health risk refers to risks which women are exposed to during pregnancy, childbirth and postnatal period. These risks can involve blood loss, anemia, obstructed labor or even death in severe cases. In 2020, maternal mortality accounted for 152 deaths per 100,000 live births globally. 

Most of these risks associated with maternal health can be prevented with timely management of resources and skills. Therefore, it is critical to expand efforts to study relationship between predictors of risks and maternal injury, disability and mortality so that women and their children can be ensured to reach their full potential for health and well-being. 

In this report we document the case of Maternal health risk in context of rural Bangladesh. We’ll use Maternal health risk data set, contributed by Daffodil International University in Dhaka, Bangladesh, from UCI Machine Learning Repository.

The data is collected through Internet of Things (IoT) based risk monitoring system from various hospitals, community clinics and maternal health care facilities from rural regions of Bangladesh. Pregnant women were effectively monitored for risk indicators with Wearable sensing technology. These indicators were then evaluated to identify risk intensity level during pregnancy. 

The data set stores values for following predictors of risk:

Age: Age in years when a woman is pregnant
SystolicBP: Upper value of Blood Pressure in mmHg
DiastolicBP: Lower value of Blood Pressure in mmHg
BS: Blood glucose levels in terms of a molar concentration in mmol/L
BodyTemp: body temperature in degrees Fahrenheit (°F).
HeartRate: A normal resting heart rate in beats per minute

And outcome of these predictors as

Risk Level: Predicted Risk Intensity Level during pregnancy considering the previous attribute





## Project Workflow 

This project will follow following structure:

#	Import Data set
We’ll import Maternal Health Risk data set from University of California Irvine Machine Learning Repository in to RStudio interface.
#	Clean Data set

Then we’ll look in to structure of data set and make sure there are no missing values for any of predictors and outcome. If none are found, we’ll divide Maternal Health Risk data set in to two sets: dat and validation set.  

dat set will contain 90% of original data while Validation set will have 10% entries of original set. The latter will only be used to test final algorithm.

Since dat set has to be partitioned further to train and test accuracy of at least five machine learning algorithms with tuning parameters, we chose to have 90:10 data partition rule. We need sufficiently enough data points to train our algorithms and tune their parameters. 

# Explore and visually analyse Data set
Bar graphs, density plots, correlation plots and other visual and statistical representations of data will aid us in understanding features of data set. 

#	Model Data set

We’ll model data set based on insights gained in data exploration. To model, we’ll generate two more data sets from dat set: Train set and Test set. 90% of dat entries will be in Train set while Test set will hold only 10% of dat entries. We’ll predict Risk Level with Test set and evaluate accuracy of the models. 

# Analyse Results
Results obtained through all the models that will be applied to train data set will be analyzed to find maximum Accuracy. The method of modelling with highest accuracy will then be applied to final set of data (dat set) to train and validate.  

#Report results. 

Final results will be reported to form conclusion and observe limitations. 


# Methods & Analysis 

#Data Cleaning 

We’ll begin with installing and loading all required libraries. 


```{r message=FALSE, warning=FALSE}
# Install all required libraries 


if(!require(readr)) install.packages("readr") 
if(!require(tidyverse)) install.packages("tidyverse") 
if(!require(caret)) install.packages("caret") 
if(!require(dplyr)) install.packages("dplyr") 
if(!require(matrixStats)) install.packages("matrixStats") 
if(!require(xgboost)) install.packages("xgboost") 
if(!require(Ckmeans.1d.dp)) install.packages("Ckmeans.1d.dp") 
if(!require(ggridges)) install.packages("ggridges") 
if(!require(patchwork)) install.packages("patchwork") 
if(!require(corrplot)) install.packages("corrplot") 
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(GGally)) install.packages("GGally")

```

```{r message=FALSE, warning=FALSE}
# load all required libraries 

library(readr)
library(tidyverse)
library(caret)
library(dplyr)
library(matrixStats)
library(xgboost)
library(Ckmeans.1d.dp)
library(ggridges)
library(patchwork)
library(corrplot)
library(ggplot2)
library(GGally)


```

Then we’ll load Maternal Health Risk data set from UCI Machine Learning Repository. 

```{r}
# load Maternal Health Risk data set from UCI Machine Learning Repository 

Data <- read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00639/Maternal%20Health%20Risk%20Data%20Set.csv")


```
The Data set has 1014 rows and 7 columns. 

```{r}
# Number of rows and columns 

dim(Data)
```
Let’s look in to first few entries of Data set.  

```{r}
# view first six entries of Data set  
head(Data)
```

Now we’ll look for missing values in Data set. 

```{r}
# check missing values in Data set
sapply(Data, function(x) sum(is.na(x)))
```
There are no missing values. 

We’ll split Data set in to two sets: dat and validation set.  dat set will contain 90% of original data while Validation set will have 10% entries of original set. The latter will only be used to test final algorithm.

Since dat set has to be partitioned further to train and test accuracy of at least five machine learning algorithms with tuning parameters, we chose to have 90:10 data partition rule. We need sufficiently enough data points to train our algorithms and tune their parameters. 

```{r}
# split the "Data set" in to train set(dat) and test set(Validation)

set.seed(1, sample.kind="Rounding")

Validation_index <- createDataPartition(y = Data$RiskLevel, times = 1, p = 0.1, list = FALSE)

dat <- Data[-Validation_index,]
dat_x <- dat[,-7]
dat_y <- dat$RiskLevel


Validation <- Data[Validation_index,]
val_x <- Validation[,-7]
val_y <- Validation$RiskLevel
```

We’ll generate two more data sets from dat set: Train set and Test set. 90% of dat entries will be in Train set while Test set will hold only 10% of dat entries. We’ll predict Risk Level with Test set and evaluate accuracy of the models.

```{r}
# Further split "dat set" in to train set and test set 

test_index <- createDataPartition(y = dat_y, times = 1, p = 0.1, list = FALSE)

train <- dat[-test_index,]
train_x <- train[,-7]
train_y <- train$RiskLevel


test <- dat[test_index,]
test_x <- test[,-7]
test_y <- test$RiskLevel

```

# Data Exploration 

Exploratory analysis will aid in understanding data set’s construct. It will help summarize data set’s main characteristics. And discover patterns and visualize these patterns.

We’ll begin with observing data set’s dimension.


```{r}
# Number of rows and columns in dat 

dim(dat)
```
There are 911 rows and 7 columns. 

All predictors in data set are numeric. These include Age, SystolicBP, DiastolicBP, BS, BodyTemp, and HeartRate. The outcome of these predictors, RiskLevel, is of character class.

```{r}
# Structure of dat
str(dat)

```

# RiskLevel 

We’ll begin with looking into RiskLevel.

RiskLevel is classified in to three levels according to intensity of risk. The samples of maternal health are determined to be either high risk, low risk or mid risk. 



```{r}


# type of RiskLevel
  
levels(factor(dat$RiskLevel))


```

```{r}
# count of each RiskLevel 

table(dat$RiskLevel) 
```
```{r}
# Visualize RisKLevel distribution 

dat %>% ggplot(aes(RiskLevel, 
                   fill = RiskLevel)) +  geom_bar(width = 1) + coord_polar(theta = "x")

```
Most samples are of low-risk patients.


# Age 

The minimum age of patients in data set is 10 years. This deviation is consistent with literature provided by UNICEF. 

UNICEF documents that Bangladesh has the highest prevalence of child marriage in South Asia. It is home to 42 million child brides. Of these, 21 million are married before age 15.   

The maximum age in maternal health sample is 70 years. Although only one such case is reported in data set. Medical findings suggest that situations like these are very less likely to occur but are not impossible with the help of modern fertility treatment. 

Moreover, high fertility rates in Bangladesh are supported with its low contraceptive prevalence rate. According to annual World Bank report, Bangladesh’s contraceptive prevalence for 2020 was just 62.5%.



```{r}


# summary statistics of Age Distribution
  
summary(dat$Age)
```
Here we represented Age distribution with bar graph, density plots and boxplot for each risk level. 

```{r}
# plot Bar Graph

A1 <- dat %>%  ggplot( aes(Age,  fill = RiskLevel)) +
  geom_bar() + facet_grid(RiskLevel~ . )


# plot Density Plot

A2 <- dat %>%  ggplot( aes(Age,  RiskLevel, fill = RiskLevel)) +
  geom_density_ridges(alpha = 0.2) 

# plot Boxplot
A3 <- dat %>%  ggplot( aes(RiskLevel, Age,  fill = RiskLevel)) +
  geom_boxplot() +
  theme(axis.text.x = element_blank())


# display these plots together 

( A1 | (A2 / A3) ) + plot_annotation(title = "Age Distribution")

```

For low risk and mid risk samples, we observe that distributions are asymmetrical and right-skewed.

The density plot for high-risk samples tend to follow a bell-shaped curve.  

The median age for high-risk cases is approximately 35 years. And for mid risk samples it is 25 years.


# SystolicBP

SystolicBP, upper value of Blood Pressure, has a median of 120 mmHg.

It has a trimodal distribution across high-risk samples and bimodal distribution across mid risk samples.

The maximum value recorded for SystolicBP was 160 mmHg which was in the case of high risk samples.

```{r}

  
# summary statistics of SystolicBP distribution
  
summary(dat$SystolicBP)
```

```{r}
# plot Bar Graph

S1 <- dat %>%  ggplot( aes(SystolicBP,  fill = RiskLevel)) +
  geom_bar() + facet_grid(RiskLevel~ . )


# plot Density Plot

S2 <- dat %>%  ggplot( aes(SystolicBP, RiskLevel, fill = RiskLevel)) +
  geom_density_ridges(alpha = 0.2) 


# plot  Boxplot 

S3 <- dat %>%  ggplot( aes(RiskLevel, SystolicBP,  fill = RiskLevel)) +
  geom_boxplot() +
  theme(axis.text.x = element_blank())


# display these plots together

(S1 | (S2 / S3)) + plot_annotation(title = "SystolicBP Distribution")

```
# DiastolicBP

The lower value of Blood Pressure is referred to as DiastolicBP. An average of 76.37 mmHg is recorded in data set. High risk samples tend to score more on this index. DiastolicBP distribution for all risk levels is not symmetrical.  



```{r}


# summary statistics of DiastolicBP Distribution
  
summary(dat$DiastolicBP)
```

```{r}
# plot Bar Graph

D1 <- dat %>%  ggplot( aes(DiastolicBP,  fill = RiskLevel)) +
  geom_bar() + facet_grid(RiskLevel~ . )


# plot Density plot


D2 <- dat %>%  ggplot( aes(DiastolicBP, RiskLevel, fill = RiskLevel)) +
  geom_density_ridges(alpha = 0.2) 


# plot Boxplot 

D3 <- dat %>%  ggplot( aes(RiskLevel, DiastolicBP,  fill = RiskLevel)) +
  geom_boxplot() +
  theme(axis.text.x = element_blank())


# display these plots together 

(D1 | (D2 / D3)) + plot_annotation(title = "DiastolicBP Distribution")


```
# BS

Blood glucose levels in terms of a molar concentration is termed as BS in data set. For low risk and mid risk samples we observe its distribution is right-skewed and asymmetrical. High risk samples compared to other risk levels score more on this index. A median of 7.5 mmol/L is reported for data set.


```{r}

# summary of BS

summary(dat$BS)

```

```{r message=FALSE, warning=FALSE}
# plot Bar Graph

B1 <- dat %>%  ggplot( aes(BS,  fill = RiskLevel)) +
  geom_bar(width = 0.1) + 
  facet_grid(RiskLevel~ ., scales = "free")


# plot Density Plot

B2 <- dat %>%  ggplot( aes(BS, RiskLevel,  fill = RiskLevel)) +
  geom_density_ridges(alpha = 0.2) 


# plot Boxplot

B3 <- dat %>%  ggplot( aes(RiskLevel, BS,  fill = RiskLevel)) +
  geom_boxplot() +
  theme(axis.text.x = element_blank())


# display these plots together

( B1 | (B2 / B3)) + plot_annotation(title = "BS Distribution")


```
# BodyTemp

The body temperature of samples were recorded in Fahrenheit. It ranges from 98 F to 103 F with an average of 98.67. 

We observe asymmetrical and right-skewed distribution for all risk levels.


```{r}

# summary statistics of BodyTemp Distribution 
  
summary(dat$BodyTemp)
```
```{r}
# plot Bar Graph

T1 <- dat %>%  ggplot( aes(BodyTemp,  fill = RiskLevel)) +
  geom_bar() + facet_grid(RiskLevel~ . )


# plot Density Plot

T2 <-  dat %>%  ggplot( aes(BodyTemp, RiskLevel, fill = RiskLevel)) +
  geom_density_ridges(alpha = 0.2)


# plot Boxplot

T3 <- dat %>%  ggplot( aes(RiskLevel, BodyTemp,  fill = RiskLevel)) +
  geom_boxplot() +
  theme(axis.text.x = element_blank())


# display these plots together 

(T1 | (T2 / T3)) + plot_annotation(title = "BodyTemp Distribution")


```
# HeartRate

Data set reports normal resting heart rate in beats per minute in HeartRate index. We observe asymmetrical distribution for the index across all risk levels. High risk samples report higher median HeartRate relative to other risk levels.  

```{r}

# summary statistics of HeartRate Distribution
  
summary(dat$HeartRate)

```
```{r}
# plot Bar Graph

H1 <- dat %>%  ggplot( aes(HeartRate,  fill = RiskLevel)) +
  geom_bar() + facet_grid(RiskLevel~ . )


# plot Density Plot

H2 <- dat %>%  ggplot( aes(HeartRate, RiskLevel,  fill = RiskLevel)) +
  geom_density_ridges(alpha = 0.2) 

# plot Boxplot

H3 <- dat %>%  ggplot( aes(RiskLevel, HeartRate,  fill = RiskLevel)) +
  geom_boxplot() +
  theme(axis.text.x = element_blank())

# display these plots together 
(H1 | (H2 / H3)) + plot_annotation(title = "HeartRate Distribution")

```

# Data Visualization & Analysis

# Heatmap 

To discover patterns or clusters in our Maternal Health risk data set, we plot the Heatmap. We centered the observations and then scaled them. The distance of these scaled observations is visually represented in the image. 



```{r}
###################
#Heat map
###################

# convert into matrix 

m <- as.matrix(dat_x)


# center entries 

x_centered <- sweep(m, 2, colMeans(m))


# scale entries 

x_scaled <- sweep(x_centered, 2, colSds(x_centered), FUN = "/")


# calculate distance 
d <- dist(x_scaled)


# plot heatmap 
heatmap(as.matrix(d), labRow = NA, labCol = NA)

```
# Correlation Heatmap and Hierarchical Clustering  

To understand how predictors relate to other predictors in data set, we plot Correlation Heatmap. The method of correlation is Spearman. That is because we observed outlier values for predictors.

The predictors are ordered with respect to Hierarchical Clustering in to two groups. The average distance between the features determined the clusters.

BS, Age, SystolicBP and DiastolicBP are all positively related to each other. Of these predictors, SystolicBP and DiastolicBP are relatively more correlated. 

Features with negative value for correlations are shaded. BodyTemp is negatively correlated to most features except for HeartRate.

```{r}
##################################################
# Correlation Heat map & Hierarchical Clustering
##################################################


# determine correlation between predictors 

x <- cor(dat_x, method = "spearman")

# plot correlation matrix 
# Hierarchical Clustering is based on average distance of features

corrplot(x, method = 'shade', order = 'hclust', addrect = 2, hclust.method = "average",
         addCoef.col ='black', col = COL2('PiYG', 10),tl.col = 'black' )

```

# Scatter Matrix 

The upper pane of Scatter Matrix shows pairwise spearman correlation index for each predictor and also for each risk level.  The lower pane of matrix displays scatter plots of these predictors.  


```{r message=FALSE, warning=FALSE}
##################
# Scatter Matrix  
##################


# plot scatter matrix 

ggpairs(dat_x,
        aes(color = dat$RiskLevel ,  
            alpha = 0.5), upper = list(continuous = wrap("cor", method= "spearman"))) 


```

# Principal Component Analysis 

Principal component analysis (PCA) is used to reduce the dimensionality of large data sets. This data analysis technique increases large data sets interpretability without the loss of the information. 


```{r}
##############################
#Principal Component Analysis
##############################


x <- as.matrix(dat_x)


# center entries 

x_centered <- sweep(x, 2, colMeans(x))


# scale entries 

x_scaled <- sweep(x_centered, 2, colSds(x_centered), FUN = "/")


# calculate distance

d <- dist(x_scaled)
```

To visualize distance vector, we plot this image.

```{r}
# visualize distance 

image(as.matrix(d), col = rev(RColorBrewer::brewer.pal(9, "PiYG")))
```


```{r}
# perform principal component analysis

pca <- prcomp(x_scaled)

summary(pca)

```

More than 50% of data is explained with just two principal components.




Standard Deviation of Principal Components reduce with each successive component.   

```{r}


# plot Standard Deviation 

plot(pca$sdev, xlab = "PC")


```
```{r}
# calculate  Variance Explained 

var_explained <- cumsum(pca$sdev^2/sum(pca$sdev^2))


# plot Varaince Explained 

plot(var_explained, xlab = "PC") 

```
The first two components report 62.4% of total variability in data set.

```{r}
# Plot PC1 & PC2  

data.frame(pca$x[,1:2], type = dat_y) %>%
  ggplot(aes(PC1, PC2, fill = type)) +
  geom_point(cex=3, pch=21)+
  ggtitle(" PC1 & PC2 Plot") + 
  coord_fixed(ratio = 1)
```
High-risk samples tend to have larger values of PC1 than low-risk and mid-risk samples.

```{r}
# Plot Principal Component Boxplot 

data.frame(type = dat_y, pca$x) %>%
      gather(key = "PC", value = "value", -type) %>%
      ggplot(aes(PC, value, fill = type)) + 
      geom_boxplot() +
      ggtitle("Principal Component Boxplot ") 
```

The inter-quartile range of high-risk samples for PC1 is larger than for any other component.

# Data Modeling 

This report will use five Machine Learning algorithms. Here is a brief description of the models. 

# Quadratic Discriminant Analysis (QDA)

A quadratic discriminant analysis is widely used classification technique. It classifies a target variable from a class of independent variables into two or more classes that is multi class classification. This technique generalizes the linear discriminant analysis (LDA) classifier to the case of distinct covariance matrices among classes. A disadvantage of QDA is that it cannot be used as a dimensionality reduction technique.

# Linear Discriminant Analysis (LDA) 

Linear Discriminant Analysis is a dimensionality reduction technique that is commonly used for supervised classification problems. The approach models the differences among samples assigned to certain groups. 
The aim of the method is to maximize the ratio of the between-group variance and the within-group variance. When the value of this ratio is at its maximum, then the samples within each group have the smallest possible scatter and the groups are separated from one another the most. LDA projects the features in higher dimensional space onto a lower-dimensional space in order to avoid the curse of dimensionality.

# K -Nearest Neighbors  

The K-Nearest Neighbors algorithm or k-NN is a supervised learning classifier. It uses proximity to classify or predict the group of an individual data point. The algorithm can be used for both Regression and Classification problems. 

K-NN is easy to implement. There are only two parameters required to implement model: value of K and the distance function. As new training samples are added, the algorithm easily adjusts to account for any new data since all training data is stored into memory. However, the model doesn’t perform well with high-dimensional data inputs and is prone to overfitting. It takes up more memory and data storage compared to other classifiers. 

# Random Forrest 

Random forest is a supervised learning algorithm that is used widely in Classification and Regression problems. It combines the output of multiple decision trees to reach a single result. The algorithm’s three prime parameters are node size, number of trees, and number of features sampled.
Few key advantages of Random Forrest are that it reduces the risk of overfitting and makes it easy to evaluate variable importance in the model. However, algorithm presents some challenges as well. Random Forrest algorithm is a time-consuming process. And with large data sets, it requires more resources to store data. Moreover, since it averages a lot of decision trees, the model loses interpretability.

# Extreme Gradient Boosting – (XGBoost) 

Extreme Gradient boosting is an extremely popular machine learning algorithm. Whereas random forests build an ensemble of deep independent trees, Extreme Gradient boosting algorithm builds an ensemble of shallow trees in sequence with each tree learning and improving on the previous one. Although shallow trees by themselves are rather weak predictive models, they can be boosted to produce a powerful “committee”.
XGBoost is an optimized distributed gradient boosting library. It offers regularization hyperparameters that curtail overfitting. And allows users to define and optimize gradient boosting models using custom objective and evaluation criteria. A user can train an XGBoost model, save the results, and later on return to that model and continue building onto the results. It also implements early stopping so that model assessment is stopped when additional trees offer no improvement.

# Results 

# Quadratic Discriminant Analysis (QDA)


```{r message=FALSE, warning=FALSE}

# Model 1:  Quadratic Discriminant Analysis (QDA)
set.seed(1, sample.kind="Rounding")
  
# train model
  
train_qda <- train(train_x, train_y, method = "qda")


# calculate predictions 
qda_preds <- predict(train_qda, test_x)


# Confusion Matrix and Statistics

confusionMatrix(data = qda_preds, reference = factor(test_y))
```


```{r}
# calculate accuracy

qda_result <- mean(qda_preds == factor(test_y))


Result <- data.frame(Model = "Quadratic Discriminant Analysis (QDA)", Accuracy = qda_result )

Result
```
With Quadratic Discriminant Analysis, accuracy was just 66.6 %.  Sensitivity for mid-risk samples was 29.03 %. The model didn’t perform well.  

# Linear Discriminant Analysis (LDA)


```{r message=FALSE, warning=FALSE}

# Model 2: Linear Discriminant Analysis (LDA)


# train model
  
train_lda <- train(train_x, train_y, method = "lda")


# calculate predictions

lda_preds <- predict(train_lda, test_x)

# Confusion Matrix and Statistics

confusionMatrix(data = lda_preds, reference = factor(test_y))

```

```{r}
# calculate accuracy 

lda_result <- mean(lda_preds == factor(test_y))

Result <- bind_rows(Result, tibble(Model = "Linear Discriminant Analysis (LDA)",
                                   Accuracy = lda_result))
Result
```
The model didn’t improve accuracy significantly. The Specificity for low-risk samples was 69.64% while Sensitivity for mid-risk samples was 32.36%. 


# K - Nearest Neighbors  
```{r message=FALSE, warning=FALSE}

# Model 3: K - Nearest Neighbors 


# define tuning parameter k
  
tuning <- data.frame(k = seq(3,27,2))

# train model

train_knn <- train(train_x, train_y, method = "knn", 
                   tuneGrid = tuning)
# find best tune 
train_knn$bestTune
```

```{r}
# plot accuracy for each k
ggplot(train_knn, highlight = TRUE) + ggtitle("K - Nearest Neighbors Accuracy") + theme_bw()

```
Maximum accuracy is achieved when k is equal to 3. As more neighbors were added, accuracy reduced. 


```{r}
# plot error bar for each k
train_knn$results %>% 
  ggplot(aes(x = k, y = Accuracy)) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(x = k, 
                    ymin = Accuracy - AccuracySD,
                    ymax = Accuracy + AccuracySD)) + ggtitle("K - Nearest Neighbors Error Bar") +
                    theme_bw()
```

```{r}
# training set outcome distribution of final model

train_knn$finalModel
```

```{r}
# calculate predictions 

knn_preds <- predict(train_knn, test_x )


# Confusion Matrix and Statistics
confusionMatrix(knn_preds, factor(test_y))

```

```{r}
# calculate accuracy 
knn_result <- mean(knn_preds == factor(test_y))

Result <- bind_rows(Result, tibble(Model = "K - Nearest Neighbors",
                                   Accuracy = knn_result))
 

Result
```

The accuracy improved by approximately 5% from Linear Discriminant Analysis. 

# Random Forrest 

```{r message=FALSE, warning=FALSE}


#Model 4: Random Forest  

# define tuning parameter - number of variables randomly sampled as candidates at each split 

tuning <- data.frame(mtry = c(3, 5, 7))

# train model
train_rf <- train(train_x, train_y, method = "rf", tuneGrid = tuning,
             importance = TRUE)


# find best tune 

train_rf$bestTune
```

```{r}
# calculate predictions 

rf_preds <- predict(train_rf, test_x)

#Confusion Matrix and Statistics

confusionMatrix(rf_preds, factor(test_y))
```

```{r message=FALSE, warning=FALSE}
# calculate accuracy 
rf_result <- mean(rf_preds == factor(test_y))


# evaluate Variable Importance 

varImp(train_rf)
```

```{r}
# plot Variable Importance 

plot(varImp(train_rf))
```
Variables are sorted by maximum importance across the risk intensity levels. BS and Age are two most important features in determining risk intensity.

```{r}
Result <- bind_rows(Result, tibble(Model = "Random Forrest",
                                   Accuracy = rf_result))
Result

```
So far, highest accuracy is achieved with Random Forrest approach. It made a significant improvement of 10% from accuracy that was estimated with K-NN approach.  

# Extreme Gradient Boosting – (XGBoost)

```{r message=FALSE, warning=FALSE}

# Model 5: Extreme Gradient Boosting (xgboost)


# convert train data in to required class
  
train_x <- as.matrix(train_x)
train_y <- as.integer(factor(train$RiskLevel)) - 1

# convert test data in to required class

test_x <- as.matrix(test_x)
test_y <- as.integer(factor(test$RiskLevel)) - 1


#  define train data input as dense matrix

xgb_train <- xgb.DMatrix(data = train_x, label = train_y)


# define test data input as dense matrix

xgb_test <- xgb.DMatrix(data = test_x, label = test_y)


# tune parameters
# gbtree - tree is grown one after other and attempts to reduce misclassification rate in subsequent iterations
# objective is multiclassification using softmax objective which returns predicted class probabilities
# evaluate model's accuracy with  multiclass logloss function

xgb_params <- list(
  booster = "gbtree",
  objective = "multi:softprob",
  eval_metric = "mlogloss",
  num_class = length(levels(factor(train$RiskLevel))))

# train model

xgb_model <- xgb.train(
  params = xgb_params,
  data = xgb_train,
  verbose = 1,
  nrounds = 200)


# calculate predictions 

xgb_preds <- predict(xgb_model, as.matrix(test_x), reshape = TRUE)

xgb_preds <- as.data.frame(xgb_preds)

colnames(xgb_preds) <- levels(factor(test$RiskLevel))


# determine Predicted Risk

Predicted_Risk <- apply(xgb_preds, 1, function(y) colnames(xgb_preds)[which.max(y)])

# determine Actual Risk

Actual_Risk <- levels(factor(test$RiskLevel))[test_y + 1]


# Confusion Matrix and Statistics

confusionMatrix(factor(Predicted_Risk), factor(Actual_Risk))

```
```{r}
# calculate accuracy 

xgb_result <- mean(Predicted_Risk == Actual_Risk)


# determine feature importance 

importance_matrix <- xgb.importance(feature_names = colnames(xgb_train), model = xgb_model)

importance_matrix
```

To plot feature importance, we’ll order variables in their relative importance from most important predictor. 

```{r}
# plot feature importance 

xgb.ggplot.importance(importance_matrix,measure = "Frequency",rel_to_first = TRUE)

```
The plot shows that BS is most important predictor followed by Age. The least important predictor in determining risk level is BodyTemp.  

```{r}
Result <- bind_rows(Result, tibble(Model = "Extreme Gradient Boosting (xgboost)",
                                   Accuracy = xgb_result))
Result
```
We estimated Risk levels with several Machine Learning algorithms. The best accuracy is achieved with Extreme Gradient Boosting. 85% of data set was correctly predicted. Therefore, we’ll use this approach to train and test our final model. 

# Final Model - Extreme Gradient Boosting – (XGBoost)

```{r}
# convert dat data in to required class  
  
dat_x <- as.matrix(dat_x)
dat_y <- as.integer(factor(dat$RiskLevel)) - 1



# convert Validation data in to required class

val_x <- as.matrix(val_x)
val_y <- as.integer(factor(Validation$RiskLevel)) - 1


# define dat data input as dense matrix

xgb_train <- xgb.DMatrix(data = dat_x, label = dat_y)

# define Validation data input as dense matrix

xgb_test <- xgb.DMatrix(data = val_x, label = val_y)

# tune parameters
# gbtree - tree is grown one after other and attempts to reduce misclassification rate in subsequent iterations
# objective is multiclassification using softmax objective which returns predicted class probabilities
# evaluate model's accuracy with  multiclass logloss function

xgb_params <- list(
  booster = "gbtree",
  objective = "multi:softprob",
  eval_metric = "mlogloss",
  num_class = length(levels(factor(dat$RiskLevel))))

# train model

xgb_model <- xgb.train(
  params = xgb_params,
  data = xgb_train,
  verbose = 1,
  nrounds = 200)


# calculate predictions 

xgb_preds <- predict(xgb_model, as.matrix(val_x), reshape = TRUE)

xgb_preds <- as.data.frame(xgb_preds)

colnames(xgb_preds) <- levels(factor(Validation$RiskLevel))

# determine Predicted Risk

Predicted_Risk <- apply(xgb_preds, 1, function(y) colnames(xgb_preds)[which.max(y)])

# determine Actual Risk

Actual_Risk <- levels(factor(Validation$RiskLevel))[val_y + 1]


# Confusion Matrix and Statistics

confusionMatrix(factor(Predicted_Risk), factor(Actual_Risk))

```
```{r}
# calculate accuracy 

Final_xgb_result <- mean(Predicted_Risk == Actual_Risk)
```

We list features ordered by their importance. 

```{r}
# determine feature importance

importance_matrix <- xgb.importance(feature_names = colnames(xgb_train), model = xgb_model)

importance_matrix
```


To plot feature importance, we’ll order variables in their relative importance from most important predictor. 

```{r}
# plot feature importance 

xgb.ggplot.importance(importance_matrix,measure = "Frequency",rel_to_first = TRUE)

```
The plot shows that BS is most important predictor followed by Age. The least important predictor in determining risk level is BodyTemp.  


```{r}
Result <- bind_rows(Result, tibble(Model = "Final Model - Extreme Gradient Boosting (xgboost)",
                                   Accuracy = Final_xgb_result))
Result
```
Our final model accomplished an accuracy of 87.37%. 


#	 Conclusion 

The report documented insights from modeling Maternal Health Risk Data with several Machine Learning algorithms. We began with Quadratic Discriminant Analysis. The classification technique achieved just 66.6% accuracy in predicting risk. We then used dimensionality reduction technique – Linear Discriminant Analysis to improve our estimate. With no significant gain in our results, a supervised learning classifier approach, K-NN, was adopted. The approach accomplished a 73% accuracy.  In pursuit of achieving best accuracy, risk levels were also predicted with Random Forest approach. Finally, with Extreme Gradient Boosting – (XGBoost) we achieved our best accuracy.  


